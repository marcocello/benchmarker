suite:
  name: "Multi-Provider Comparison Benchmark"
  description: "Compare Azure OpenAI and Anthropic Claude on the same dataset"

  # Test dataset
  dataset: "data/datasets/simple_qa.yaml"

  scenarios:
    - name: "qa_test_azure_gpt4o"
      strategy: "direct_prompt"
      provider: "azure_openai"
    
    - name: "qa_test_claude_sonnet"
      strategy: "direct_prompt"
      provider: "anthropic"

  # Scoring configuration using Claude as judge
  scoring:
    name: "claude_judge"
    strategy: "llm_judge"
    provider: "anthropic"
    max_score: 10
    prompt: |
      You are evaluating the quality of an answer to a factual question.
      
      Question: {question}
      Expected Answer: {expected_answer}
      Actual Answer: {response}
      
      Rate the answer on a scale of 0-10 where:
      - 10: Perfect, accurate answer
      - 8-9: Mostly correct with minor issues
      - 6-7: Partially correct
      - 4-5: Some relevant information but mostly incorrect
      - 0-3: Completely wrong or irrelevant
      
      Respond with a JSON object containing 'score' (number) and 'explanation' (string).
    scoring:
      - "Factual accuracy"
      - "Completeness of answer"
      - "Clarity and coherence"

  # Output settings
  output:
    name: multi_provider_comparison.json

  providers:
    azure_openai:
      type: azure_openai
      endpoint: ${AZURE_OPENAI_ENDPOINT}
      api_version: ${AZURE_OPENAI_API_VERSION}
      api_key: ${AZURE_OPENAI_API_KEY}
      deployment: ${AZURE_OPENAI_DEPLOYMENT}
      model: ${AZURE_OPENAI_MODEL}

    anthropic:
      type: anthropic
      api_key: ${ANTHROPIC_API_KEY}
      model: claude-3-5-sonnet-20241022
      defaults:
        temperature: 0.1
        max_tokens: 500
